import time

from pyspark.sql import SparkSession


def create_spark_session():
    """–°–æ–∑–¥–∞–µ—Ç Spark —Å–µ—Å—Å–∏—é —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    max_retries = 3
    retry_delay = 10  # —Å–µ–∫—É–Ω–¥—ã

    for attempt in range(max_retries):
        try:
            spark = (
                SparkSession.builder.appName("DockerSparkApp")
                .master("spark://localhost:7077")
                .config("spark.executor.memory", "1g")
                .config("spark.driver.memory", "1g")
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                .getOrCreate()
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Spark –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
            spark.sparkContext.parallelize([1, 2, 3, 4, 5]).count()
            print("‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Spark cluster!")
            return spark

        except Exception as e:
            print(f"‚ùå –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            if attempt < max_retries - 1:
                print(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫—É–Ω–¥...")
                time.sleep(retry_delay)
            else:
                raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Spark –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
try:
    spark = create_spark_session()

    # –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
    df = spark.range(1000).toDF("id")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {df.count()}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ DataFrame
    sample_data = [
        ("IT", "–ú–æ—Å–∫–≤–∞", 5000),
        ("Finance", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", 4500),
        ("HR", "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", 3500),
        ("IT", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", 4800),
        ("Marketing", "–ö–∞–∑–∞–Ω—å", 4000),
    ]

    employees_df = spark.createDataFrame(sample_data, ["department", "city", "salary"])

    print("–î–∞–Ω–Ω—ã–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤:")
    employees_df.show()

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("–°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞ –ø–æ –æ—Ç–¥–µ–ª–∞–º:")
    employees_df.groupBy("department").avg("salary").show()

    spark.stop()

except Exception as e:
    print(f"–û—à–∏–±–∫–∞: {e}")
